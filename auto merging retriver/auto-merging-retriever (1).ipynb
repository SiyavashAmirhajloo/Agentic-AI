{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"cefbbbe553b645698eaa8dda7c3008d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_998f3fd94aaa4298a0069d2d5d420b07","IPY_MODEL_dbb18e8fe1774513bb27cbc0136da538","IPY_MODEL_3a3bee5d4dff48ad8022c0d8b3a30cb7"],"layout":"IPY_MODEL_ab8e7ebdab064f218aa88e9ff30e0f8a"}},"998f3fd94aaa4298a0069d2d5d420b07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2fbbb1561c345368d787986b6afe5be","placeholder":"​","style":"IPY_MODEL_79d52007d4ee4f099e46890bfb6660bc","value":"modules.json: 100%"}},"dbb18e8fe1774513bb27cbc0136da538":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ec262d71cef453fa3d382872c2eb374","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9af898ce959f4c8a970634fd00bd582c","value":349}},"3a3bee5d4dff48ad8022c0d8b3a30cb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f7cd8f2a964d1896929fe0976de890","placeholder":"​","style":"IPY_MODEL_4f376c3e0fad41f382a548e13863f00d","value":" 349/349 [00:00&lt;00:00, 26.4kB/s]"}},"ab8e7ebdab064f218aa88e9ff30e0f8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2fbbb1561c345368d787986b6afe5be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d52007d4ee4f099e46890bfb6660bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ec262d71cef453fa3d382872c2eb374":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af898ce959f4c8a970634fd00bd582c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01f7cd8f2a964d1896929fe0976de890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f376c3e0fad41f382a548e13863f00d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3977d620ff6741cbb56483a825200d83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bcdb1464703a44a6b1be9ee7b7a74fc1","IPY_MODEL_a512aab5ab8f4c3485094a40a281e333","IPY_MODEL_51cf8aff671c45e28eecef0bec2d9dee"],"layout":"IPY_MODEL_0af7d03c0e0845fe8229cbb22c0e2e83"}},"bcdb1464703a44a6b1be9ee7b7a74fc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a02e272164784f4ea6de3845526b223f","placeholder":"​","style":"IPY_MODEL_dddaa3a0946b42ccbc92171e4628dde5","value":"config_sentence_transformers.json: 100%"}},"a512aab5ab8f4c3485094a40a281e333":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f741c47b4cbb4b6aad8604e674cf1c8f","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4271278572bf4da7bd6b0da27b2e1084","value":116}},"51cf8aff671c45e28eecef0bec2d9dee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_661fe76f6a1947c4bc29057377eb44a2","placeholder":"​","style":"IPY_MODEL_48347f58c1b640729c9f486743e37fdc","value":" 116/116 [00:00&lt;00:00, 10.5kB/s]"}},"0af7d03c0e0845fe8229cbb22c0e2e83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a02e272164784f4ea6de3845526b223f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dddaa3a0946b42ccbc92171e4628dde5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f741c47b4cbb4b6aad8604e674cf1c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4271278572bf4da7bd6b0da27b2e1084":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"661fe76f6a1947c4bc29057377eb44a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48347f58c1b640729c9f486743e37fdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b428c606d4140159e458f0255edff60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72dfcf808e4041fe985de9594b696fb3","IPY_MODEL_2511ea2c8fb84b619ba8f68491578bdf","IPY_MODEL_2ce64ae9edae4ee6979daf7863053c9a"],"layout":"IPY_MODEL_44b7cd79d8de45c79f0879016acce35c"}},"72dfcf808e4041fe985de9594b696fb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d0838c555b4960981fdde535b6d429","placeholder":"​","style":"IPY_MODEL_3c02c966c5c14fa5a3e155acbc7c6e83","value":"README.md: "}},"2511ea2c8fb84b619ba8f68491578bdf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9d94100f8004bf8a086313d72b407ea","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_477e55c510104f82ac648057d1de35ef","value":1}},"2ce64ae9edae4ee6979daf7863053c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef8dfe24622c4161bc6b039deb26abf3","placeholder":"​","style":"IPY_MODEL_426189a8dba248e295bc3fd618240097","value":" 10.5k/? [00:00&lt;00:00, 615kB/s]"}},"44b7cd79d8de45c79f0879016acce35c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d0838c555b4960981fdde535b6d429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c02c966c5c14fa5a3e155acbc7c6e83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9d94100f8004bf8a086313d72b407ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"477e55c510104f82ac648057d1de35ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef8dfe24622c4161bc6b039deb26abf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426189a8dba248e295bc3fd618240097":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd80e42e170d4f349d7f7c8112b03d04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12243ffd642646fbba24f545671b118f","IPY_MODEL_3531086edebf4f2ebf2b117757945375","IPY_MODEL_aed92eeb51e8438e81956d4d47748d96"],"layout":"IPY_MODEL_969ec5ffce6c4f9294ab56295205e200"}},"12243ffd642646fbba24f545671b118f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7d9e40dd4f4c908c1760047bacdaef","placeholder":"​","style":"IPY_MODEL_be3e6e8676024c65965702938e6756b5","value":"sentence_bert_config.json: 100%"}},"3531086edebf4f2ebf2b117757945375":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae69bcf0ccbc4ead818cce8e26df5596","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c93d7839400f439b8d95287f2bc0dff6","value":53}},"aed92eeb51e8438e81956d4d47748d96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3f26730f7074ea7961baa64b4ff5529","placeholder":"​","style":"IPY_MODEL_f2e247407bab4aa494025184886c8f29","value":" 53.0/53.0 [00:00&lt;00:00, 5.46kB/s]"}},"969ec5ffce6c4f9294ab56295205e200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7d9e40dd4f4c908c1760047bacdaef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be3e6e8676024c65965702938e6756b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae69bcf0ccbc4ead818cce8e26df5596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c93d7839400f439b8d95287f2bc0dff6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3f26730f7074ea7961baa64b4ff5529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2e247407bab4aa494025184886c8f29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79cc4d78ff394ea58653ade177337947":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ebf2eab83dba433eb6c89cd3346272f0","IPY_MODEL_dacf121b87404743a1dc553c28afc2b0","IPY_MODEL_b626cf6b716b4ebbb86170d354d13371"],"layout":"IPY_MODEL_5cb9c24e575c4baeb3d246380ef5c5ea"}},"ebf2eab83dba433eb6c89cd3346272f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c900e197c40a4384921fbaea52193cde","placeholder":"​","style":"IPY_MODEL_6abf1c02ba4d4cdca49f47e60aa5ae8a","value":"config.json: 100%"}},"dacf121b87404743a1dc553c28afc2b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd3b4b8286b43ce9552fef0095d4b02","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60320a3988c04697bd30df89b43b195b","value":612}},"b626cf6b716b4ebbb86170d354d13371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99d35e66a5d4ca785d65c9b5f5d90bb","placeholder":"​","style":"IPY_MODEL_e9103097df354084b42d661bf2f594cb","value":" 612/612 [00:00&lt;00:00, 59.5kB/s]"}},"5cb9c24e575c4baeb3d246380ef5c5ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c900e197c40a4384921fbaea52193cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6abf1c02ba4d4cdca49f47e60aa5ae8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebd3b4b8286b43ce9552fef0095d4b02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60320a3988c04697bd30df89b43b195b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e99d35e66a5d4ca785d65c9b5f5d90bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9103097df354084b42d661bf2f594cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26069878285b4f66907e3082509ebe75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68129bd9ae7e4d0abcd42306bd663cdb","IPY_MODEL_6f91fa8264f44747b17a80f8b9700a86","IPY_MODEL_ca704cc2c2d5443f87ec57d1d9c978de"],"layout":"IPY_MODEL_4e33e14c02af448aa408ed977bcbcf4e"}},"68129bd9ae7e4d0abcd42306bd663cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abb0bf1c00f24b4ca9f13b4aaeb85c78","placeholder":"​","style":"IPY_MODEL_743fbda77a8a4d19be47b9f76e1959f2","value":"model.safetensors: 100%"}},"6f91fa8264f44747b17a80f8b9700a86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c6ae20f4cc241ce8b6ea1c78150c398","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32e02f3ac14943c0991569a6ed1ef21d","value":90868376}},"ca704cc2c2d5443f87ec57d1d9c978de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23c259b1d82c4194a2129d9c34504693","placeholder":"​","style":"IPY_MODEL_8fa8d25a4c4a45c498d8cf34619d871f","value":" 90.9M/90.9M [00:01&lt;00:00, 120MB/s]"}},"4e33e14c02af448aa408ed977bcbcf4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb0bf1c00f24b4ca9f13b4aaeb85c78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"743fbda77a8a4d19be47b9f76e1959f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c6ae20f4cc241ce8b6ea1c78150c398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e02f3ac14943c0991569a6ed1ef21d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23c259b1d82c4194a2129d9c34504693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fa8d25a4c4a45c498d8cf34619d871f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bd64a67b60443609fd247c73088ed83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdc4b763a2144d6ca131e2845f8b886a","IPY_MODEL_23487d12ff4a4f008a15a4281c996bbd","IPY_MODEL_44ecd1d2e5aa405c9b9107a4fe30829c"],"layout":"IPY_MODEL_1f432ff251d44b83821e106dd1a016b7"}},"fdc4b763a2144d6ca131e2845f8b886a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38d8105019b643ef8af9581e586aac30","placeholder":"​","style":"IPY_MODEL_8e5e527788534b33abf3c184610b8088","value":"tokenizer_config.json: 100%"}},"23487d12ff4a4f008a15a4281c996bbd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_465535896bd4418db215dc2058d1f75e","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ac08c0c9203400dbb52d879df29a6bd","value":350}},"44ecd1d2e5aa405c9b9107a4fe30829c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3120eed398d949e4a333f2a497297c08","placeholder":"​","style":"IPY_MODEL_3e8fca6c1f624721b5d012298d8cef96","value":" 350/350 [00:00&lt;00:00, 31.4kB/s]"}},"1f432ff251d44b83821e106dd1a016b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38d8105019b643ef8af9581e586aac30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e5e527788534b33abf3c184610b8088":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"465535896bd4418db215dc2058d1f75e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ac08c0c9203400dbb52d879df29a6bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3120eed398d949e4a333f2a497297c08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8fca6c1f624721b5d012298d8cef96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36e469a0744d464d859753e0c74be550":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_891db1d269904eaeaf93dfbb7c30999c","IPY_MODEL_b9f403017d9b48eda2494f6c5b9d3cb8","IPY_MODEL_0cfb6db8a58349cfade219485f59b2da"],"layout":"IPY_MODEL_19811d100c62463dbbc15bbb2f8bff61"}},"891db1d269904eaeaf93dfbb7c30999c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4712c83bfe1c4a4795bebe6398053637","placeholder":"​","style":"IPY_MODEL_83afa45cc9784606b8ff1df7a94de136","value":"vocab.txt: "}},"b9f403017d9b48eda2494f6c5b9d3cb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66af4157bff14b3493e157a04c6f40e0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9922a9bc0b654bab9d31f17bd7502189","value":1}},"0cfb6db8a58349cfade219485f59b2da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff9b76b75d19415f8534104e722783f9","placeholder":"​","style":"IPY_MODEL_a71d7a350f744b8facbe3e998bcc5eab","value":" 232k/? [00:00&lt;00:00, 8.16MB/s]"}},"19811d100c62463dbbc15bbb2f8bff61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4712c83bfe1c4a4795bebe6398053637":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83afa45cc9784606b8ff1df7a94de136":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66af4157bff14b3493e157a04c6f40e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9922a9bc0b654bab9d31f17bd7502189":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff9b76b75d19415f8534104e722783f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a71d7a350f744b8facbe3e998bcc5eab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a12fb73b82841e19265cb44130668c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1a01adc48d64cff85420d2e06942808","IPY_MODEL_11a5411719144431b908289a8fb83440","IPY_MODEL_3c667b6ebb4e49479a67ba74b554b65c"],"layout":"IPY_MODEL_e5fad7f07df943e4b418b5077546f7cd"}},"f1a01adc48d64cff85420d2e06942808":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d591e4ee694408296484dd598252d44","placeholder":"​","style":"IPY_MODEL_23e1ce7086a94fa8939a35ff0efcd7cd","value":"tokenizer.json: "}},"11a5411719144431b908289a8fb83440":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb31b6dfb87e481e84d04eb3032ab1cf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c03c4da096654d62acc92f3b9e926016","value":1}},"3c667b6ebb4e49479a67ba74b554b65c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03b3402747114503a720616ef11b04c5","placeholder":"​","style":"IPY_MODEL_1a7974de8c284a0896004a4207d5edd1","value":" 466k/? [00:00&lt;00:00, 20.4MB/s]"}},"e5fad7f07df943e4b418b5077546f7cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d591e4ee694408296484dd598252d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23e1ce7086a94fa8939a35ff0efcd7cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb31b6dfb87e481e84d04eb3032ab1cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c03c4da096654d62acc92f3b9e926016":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03b3402747114503a720616ef11b04c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a7974de8c284a0896004a4207d5edd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efa2c6f305974c4ebb601da810efa00d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3096aaf5fc6f4bb7863a1f05870c73fb","IPY_MODEL_6ec44078a3a046c78599724990784d5f","IPY_MODEL_ef5b23202ed14f2d92b67cbcd144b633"],"layout":"IPY_MODEL_84e5327462d94e69acb59c2d2d0a1126"}},"3096aaf5fc6f4bb7863a1f05870c73fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d35346f6f44b4d3383ebc0dd4de0cd21","placeholder":"​","style":"IPY_MODEL_0cd71651498c498e9fc46f8e8175ba8d","value":"special_tokens_map.json: 100%"}},"6ec44078a3a046c78599724990784d5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e35265a7d2704665b9685c648344f578","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9dd0dbbd0184ba48501fabec0253665","value":112}},"ef5b23202ed14f2d92b67cbcd144b633":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_131af91b7498467aa3f83529ac8556d8","placeholder":"​","style":"IPY_MODEL_0d86cc899f6746eda67d26875f99710c","value":" 112/112 [00:00&lt;00:00, 10.5kB/s]"}},"84e5327462d94e69acb59c2d2d0a1126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d35346f6f44b4d3383ebc0dd4de0cd21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cd71651498c498e9fc46f8e8175ba8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e35265a7d2704665b9685c648344f578":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9dd0dbbd0184ba48501fabec0253665":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"131af91b7498467aa3f83529ac8556d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d86cc899f6746eda67d26875f99710c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dee80b61f04a492c9bbb683169c1b04f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f714666c4f8c4ef7bcc4a362f8c1129a","IPY_MODEL_c6a68f3620a9430295141c00d83b0212","IPY_MODEL_3f9b0059fecb4f7dbaafbce6d950a551"],"layout":"IPY_MODEL_1b5a31801e9b41478e376785f0158a17"}},"f714666c4f8c4ef7bcc4a362f8c1129a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9954d2aa27f04276874ce82d6be17330","placeholder":"​","style":"IPY_MODEL_4a992513893f4b42977fd5e1529198d0","value":"config.json: 100%"}},"c6a68f3620a9430295141c00d83b0212":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b2630be685411f911033395214ee86","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e231c4ecb129463a892a93593e2a8d57","value":190}},"3f9b0059fecb4f7dbaafbce6d950a551":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90b7ee59e94e4a3c87ce3a7fc3853b85","placeholder":"​","style":"IPY_MODEL_d528c08d0e5245379f6fb094ca97f80a","value":" 190/190 [00:00&lt;00:00, 17.3kB/s]"}},"1b5a31801e9b41478e376785f0158a17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9954d2aa27f04276874ce82d6be17330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a992513893f4b42977fd5e1529198d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64b2630be685411f911033395214ee86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e231c4ecb129463a892a93593e2a8d57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90b7ee59e94e4a3c87ce3a7fc3853b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d528c08d0e5245379f6fb094ca97f80a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Auto Merging Retriever**\n\nThis notebook demonstrates the **AutoMergingRetriever**, which recursively merges leaf nodes that reference the same parent node beyond a threshold. This consolidates smaller, scattered contexts into larger ones to improve synthesis.  \n\nYou can define the hierarchy manually or use the **HierarchicalNodeParser**, which automatically creates a coarse-to-fine node hierarchy from a set of documents.\n\n---\n","metadata":{"id":"cffqpyQuUnqz"}},{"cell_type":"code","source":"# %pip install llama-index-llms-openai==0.6.5\n# %pip install llama-index-readers-file==0.5.4 pymupdf==1.26.5\n# # If you’re opening this Notebook on colab, you will probably need to install LlamaIndex 🦙.\n# !pip install llama-index==0.14.5\n# !pip install --upgrade ipython\n# !pip install llama-index-llms-huggingface-api==0.6.1\n# !pip install llama-index-embeddings-huggingface==0.6.1\n# !pip install sentence-transformers==5.1.2\n\n%pip install llama-index-llms-openai\n%pip install llama-index-readers-file pymupdf\n# If you’re opening this Notebook on colab, you will probably need to install LlamaIndex 🦙.\n!pip install llama-index\n!pip install --upgrade ipython\n!pip install llama-index-llms-huggingface-api\n!pip install llama-index-embeddings-huggingface\n!pip install sentence-transformers\n","metadata":{"id":"Vd2t9OySUl4W","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4b4196e6-7da7-401d-aff9-701ae527cc16","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:26:17.979105Z","iopub.execute_input":"2025-10-28T18:26:17.979538Z","iopub.status.idle":"2025-10-28T18:31:30.289418Z","shell.execute_reply.started":"2025-10-28T18:26:17.979498Z","shell.execute_reply":"2025-10-28T18:31:30.287838Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting llama-index-llms-openai\n  Downloading llama_index_llms_openai-0.6.6-py3-none-any.whl.metadata (3.0 kB)\nCollecting llama-index-core<0.15,>=0.14.5 (from llama-index-llms-openai)\n  Downloading llama_index_core-0.14.6-py3-none-any.whl.metadata (2.5 kB)\nCollecting openai<2,>=1.108.1 (from llama-index-llms-openai)\n  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.12.15)\nRequirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.21.0)\nCollecting banks<3,>=2.2.0 (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.18)\nCollecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.9.0)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.28.1)\nCollecting llama-index-workflows<3,>=2 (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading llama_index_workflows-2.8.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.5)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (11.3.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.12.0a1)\nRequirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.0.3)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.32.5)\nCollecting setuptools>=80.9.0 (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.0.41)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (8.5.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.9.0)\nRequirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.15.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.17.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.20.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2,>=1.108.1->llama-index-llms-openai) (3.10)\nCollecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.1.6)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.16.0)\nCollecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai)\n  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.9.18)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.5.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.2.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.26.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.4.1)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.0)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2024.2.0)\nDownloading llama_index_llms_openai-0.6.6-py3-none-any.whl (26 kB)\nDownloading llama_index_core-0.14.6-py3-none-any.whl (11.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\nDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading llama_index_workflows-2.8.3-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\nDownloading griffe-1.14.0-py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dirtyjson, setuptools, griffe, openai, llama-index-instrumentation, banks, llama-index-workflows, llama-index-core, llama-index-llms-openai\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n  Attempting uninstall: openai\n    Found existing installation: openai 1.97.1\n    Uninstalling openai-1.97.1:\n      Successfully uninstalled openai-1.97.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banks-2.2.0 dirtyjson-1.0.8 griffe-1.14.0 llama-index-core-0.14.6 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.6 llama-index-workflows-2.8.3 openai-1.109.1 setuptools-80.9.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting llama-index-readers-file\n  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\nCollecting pymupdf\n  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.13.4)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.7.1)\nRequirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.14.6)\nRequirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.3)\nRequirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (6.1.0)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (4.15.0)\nRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.12.15)\nRequirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.21.0)\nRequirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.2.0)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.2.18)\nRequirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.0.8)\nRequirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.2.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.9.0)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.28.1)\nRequirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.8.3)\nRequirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.5)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (11.3.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.12.0a1)\nRequirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (6.0.3)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.32.5)\nRequirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (80.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.0.41)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (8.5.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.9.0)\nRequirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.67.1)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.17.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.20.1)\nRequirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.14.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.1.6)\nRequirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.9.18)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.8.3)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.2.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.26.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.16.0)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (25.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.3.1)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2024.2.0)\nDownloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nInstalling collected packages: striprtf, pymupdf, llama-index-readers-file\nSuccessfully installed llama-index-readers-file-0.5.4 pymupdf-1.26.5 striprtf-0.0.26\nNote: you may need to restart the kernel to use updated packages.\nCollecting llama-index\n  Downloading llama_index-0.14.6-py3-none-any.whl.metadata (13 kB)\nCollecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: llama-index-core<0.15.0,>=0.14.6 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.14.6)\nCollecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.6)\nRequirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.4)\nCollecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\nRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (3.12.15)\nRequirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.21.0)\nRequirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.2.0)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.2.18)\nRequirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.0.8)\nRequirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.2.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2025.9.0)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.28.1)\nRequirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.8.3)\nRequirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (3.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (11.3.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.12.0a1)\nRequirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (6.0.3)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.32.5)\nRequirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (80.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.0.41)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (8.5.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.9.0)\nRequirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.15.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.17.2)\nRequirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.109.1)\nCollecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.8.3)\nRequirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.4)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\nRequirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\nRequirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.1.0)\nRequirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2025.9.18)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.20.1)\nRequirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.14.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.1.6)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.7)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.16.0)\nRequirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.2)\nCollecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.5.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.2.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.26.1)\nINFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\nINFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\nCollecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\nCollecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama-index) (25.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama-index) (2024.2.0)\nDownloading llama_index-0.14.6-py3-none-any.whl (7.4 kB)\nDownloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\nDownloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\nDownloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\nDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\nDownloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: llama-cloud, llama-cloud-services, llama-parse, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-indices-managed-llama-cloud, llama-index-cli, llama-index\nSuccessfully installed llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.6 llama-index-cli-0.5.3 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-readers-llama-parse-0.5.1 llama-parse-0.6.54\nRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\nCollecting ipython\n  Downloading ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\nRequirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/dist-packages (from ipython) (1.1.1)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython) (0.19.2)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.2)\nCollecting stack_data (from ipython)\n  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting traitlets>=5.13.0 (from ipython)\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.15.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\nCollecting executing>=1.2.0 (from stack_data->ipython)\n  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython) (3.0.0)\nCollecting pure-eval (from stack_data->ipython)\n  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nDownloading ipython-9.6.0-py3-none-any.whl (616 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\nDownloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\nDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nInstalling collected packages: pure-eval, traitlets, executing, stack_data, ipython\n  Attempting uninstall: traitlets\n    Found existing installation: traitlets 5.7.1\n    Uninstalling traitlets-5.7.1:\n      Successfully uninstalled traitlets-5.7.1\n  Attempting uninstall: ipython\n    Found existing installation: ipython 7.34.0\n    Uninstalling ipython-7.34.0:\n      Successfully uninstalled ipython-7.34.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.6.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed executing-2.2.1 ipython-9.6.0 pure-eval-0.2.3 stack_data-0.6.3 traitlets-5.14.3\nCollecting llama-index-llms-huggingface-api\n  Downloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: huggingface-hub>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface-api) (1.0.0rc2)\nRequirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface-api) (0.14.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (6.0.3)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (0.28.1)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (4.67.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (0.19.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.1.10)\nRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.12.15)\nRequirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.21.0)\nRequirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.2.0)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.2.18)\nRequirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.0.8)\nRequirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.2.0)\nRequirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.8.3)\nRequirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.5)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (11.3.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (4.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.12.0a1)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.32.5)\nRequirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (80.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.0.41)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (8.5.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.17.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.20.1)\nRequirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.14.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.1.6)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (0.16.0)\nRequirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2025.9.18)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.5.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.2.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.26.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2.4.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.3.1)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-llms-huggingface-api) (2024.2.0)\nDownloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl (7.5 kB)\nInstalling collected packages: llama-index-llms-huggingface-api\nSuccessfully installed llama-index-llms-huggingface-api-0.6.1\nCollecting llama-index-embeddings-huggingface\n  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\nRequirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.0.0rc2)\nRequirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (0.14.6)\nRequirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (4.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.3)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.28.1)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.19.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.1.10)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.12.15)\nRequirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.21.0)\nRequirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.2.0)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\nRequirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\nRequirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\nRequirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.8.3)\nRequirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.5)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (11.3.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.12.0a1)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.32.5)\nRequirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (80.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.41)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.5.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.17.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.53.3)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.20.1)\nRequirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.14.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.1.6)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.16.0)\nRequirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2025.9.18)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.5.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.2.3)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\nINFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\nCollecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n  Downloading transformers-4.55.1-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m211.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m181.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n  Downloading sentence_transformers-4.0.0-py3-none-any.whl.metadata (13 kB)\n  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\nCollecting transformers<5.0.0,>=4.38.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting transformers<5.0.0,>=4.34.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\nCollecting transformers<5.0.0,>=4.32.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub[inference]>=0.19.0 (from llama-index-embeddings-huggingface)\n  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n\u001b[33mWARNING: huggingface-hub 1.0.1 does not provide the extra 'inference'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.4)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n\u001b[33mWARNING: huggingface-hub 1.0.1 does not provide the extra 'inference'\u001b[0m\u001b[33m\n\u001b[0mCollecting huggingface-hub[inference]>=0.19.0 (from llama-index-embeddings-huggingface)\n  Downloading huggingface_hub-1.0.0-py3-none-any.whl.metadata (13 kB)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.26.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2024.2.0)\nDownloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, llama-index-embeddings-huggingface\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 llama-index-embeddings-huggingface-0.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n","metadata":{"id":"NDuvO8T2VGr2","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:06.670283Z","iopub.execute_input":"2025-10-28T18:32:06.670901Z","iopub.status.idle":"2025-10-28T18:32:06.864909Z","shell.execute_reply.started":"2025-10-28T18:32:06.670866Z","shell.execute_reply":"2025-10-28T18:32:06.864191Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load Data\n\nLet’s first load the Llama 2 paper: https://arxiv.org/pdf/2307.09288.pdf. This will be our test data.\n","metadata":{"id":"W1QFKhsZV_NG"}},{"cell_type":"code","source":"!mkdir -p 'data/'\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-dfApT_VNeh","outputId":"4a67289a-ee7a-4bfe-b8b5-0f90eef0e8f1","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:08.579824Z","iopub.execute_input":"2025-10-28T18:32:08.580504Z","iopub.status.idle":"2025-10-28T18:32:09.113384Z","shell.execute_reply.started":"2025-10-28T18:32:08.580477Z","shell.execute_reply":"2025-10-28T18:32:09.112421Z"}},"outputs":[{"name":"stdout","text":"--2025-10-28 18:32:08--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.131.42, ...\nConnecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /pdf/2307.09288 [following]\n--2025-10-28 18:32:08--  https://arxiv.org/pdf/2307.09288\nReusing existing connection to arxiv.org:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) [application/pdf]\nSaving to: ‘data/llama2.pdf’\n\ndata/llama2.pdf     100%[===================>]  13.03M  --.-KB/s    in 0.1s    \n\n2025-10-28 18:32:09 (96.7 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path\n\nfrom llama_index.readers.file import PDFReader\nfrom llama_index.readers.file import PyMuPDFReader\n","metadata":{"id":"ARwEzonFWFEP","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:10.087008Z","iopub.execute_input":"2025-10-28T18:32:10.088029Z","iopub.status.idle":"2025-10-28T18:32:13.973308Z","shell.execute_reply.started":"2025-10-28T18:32:10.087991Z","shell.execute_reply":"2025-10-28T18:32:13.972356Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"loader = PyMuPDFReader()\ndocs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))\n","metadata":{"id":"YQjAkDYxmiA8","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:17.112158Z","iopub.execute_input":"2025-10-28T18:32:17.113101Z","iopub.status.idle":"2025-10-28T18:32:18.431677Z","shell.execute_reply.started":"2025-10-28T18:32:17.113073Z","shell.execute_reply":"2025-10-28T18:32:18.430702Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"By default, the PDF reader creates a separate document for each page. Here, we combine all pages into a single document to better demonstrate the **auto-merging** capabilities that later stitch chunks together.\n","metadata":{"id":"fmGCCI7fmu1n"}},{"cell_type":"code","source":"from llama_index.core import Document\n\ndoc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\ndocs = [Document(text=doc_text)]\n","metadata":{"id":"XMTgjXtnmkBO","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:20.667485Z","iopub.execute_input":"2025-10-28T18:32:20.667789Z","iopub.status.idle":"2025-10-28T18:32:20.699833Z","shell.execute_reply.started":"2025-10-28T18:32:20.667767Z","shell.execute_reply":"2025-10-28T18:32:20.698782Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Parse Chunk Hierarchy from Text, Load into Storage\n\nIn this section, we use the **HierarchicalNodeParser** to create a hierarchy of nodes. Top-level nodes have larger chunks, while child nodes have smaller chunks, each linked to a parent node with a bigger chunk size.\n\nBy default, the hierarchy is:\n\n*   1st level: chunk size 2048\n*   2nd level: chunk size 512\n*   3rd level: chunk size 128\n\nWe load the nodes into storage, where **leaf nodes** are indexed in a vector store for direct similarity-based retrieval, and **other nodes** are stored in a docstore for hierarchical retrieval.\n","metadata":{"id":"pS_iecCUm333"}},{"cell_type":"code","source":"from llama_index.core.node_parser import (\n    HierarchicalNodeParser,\n    SentenceSplitter,\n)\n","metadata":{"id":"Mb9wA8fGmxYt","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:22.369124Z","iopub.execute_input":"2025-10-28T18:32:22.369487Z","iopub.status.idle":"2025-10-28T18:32:22.399524Z","shell.execute_reply.started":"2025-10-28T18:32:22.369463Z","shell.execute_reply":"2025-10-28T18:32:22.398689Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"node_parser = HierarchicalNodeParser.from_defaults()\n","metadata":{"id":"Zoh4h98fnUZN","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:24.496153Z","iopub.execute_input":"2025-10-28T18:32:24.496779Z","iopub.status.idle":"2025-10-28T18:32:24.788064Z","shell.execute_reply.started":"2025-10-28T18:32:24.496751Z","shell.execute_reply":"2025-10-28T18:32:24.787311Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"nodes = node_parser.get_nodes_from_documents(docs)\n","metadata":{"id":"i2rnjOERnWcn","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:24.827651Z","iopub.execute_input":"2025-10-28T18:32:24.828315Z","iopub.status.idle":"2025-10-28T18:32:27.335029Z","shell.execute_reply.started":"2025-10-28T18:32:24.828289Z","shell.execute_reply":"2025-10-28T18:32:27.334272Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(nodes)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2DxesFTnbsC","outputId":"59bb6bcc-5e42-4bfa-b6c4-3ff19223d6b8","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:32:27.393667Z","iopub.execute_input":"2025-10-28T18:32:27.394640Z","iopub.status.idle":"2025-10-28T18:32:27.452203Z","shell.execute_reply.started":"2025-10-28T18:32:27.394615Z","shell.execute_reply":"2025-10-28T18:32:27.451274Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1008"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"We import a helper function to fetch **leaf nodes**—nodes that do not have any children.\n","metadata":{"id":"YaTnYv1Qnifo"}},{"cell_type":"code","source":"from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n","metadata":{"id":"5aHIhQrLndZG","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:19.657489Z","iopub.execute_input":"2025-10-28T18:53:19.659995Z","iopub.status.idle":"2025-10-28T18:53:19.792974Z","shell.execute_reply.started":"2025-10-28T18:53:19.659921Z","shell.execute_reply":"2025-10-28T18:53:19.791707Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"leaf_nodes = get_leaf_nodes(nodes)\n","metadata":{"id":"lWggCzdCnk0M","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:19.794964Z","iopub.execute_input":"2025-10-28T18:53:19.795249Z","iopub.status.idle":"2025-10-28T18:53:19.847751Z","shell.execute_reply.started":"2025-10-28T18:53:19.795225Z","shell.execute_reply":"2025-10-28T18:53:19.846652Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"len(leaf_nodes)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4LUKeYVnmcT","outputId":"068422d0-486b-4d39-ae1e-28e508898a38","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:20.355177Z","iopub.execute_input":"2025-10-28T18:53:20.355609Z","iopub.status.idle":"2025-10-28T18:53:20.405281Z","shell.execute_reply.started":"2025-10-28T18:53:20.355582Z","shell.execute_reply":"2025-10-28T18:53:20.403990Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"782"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"root_nodes = get_root_nodes(nodes)\n","metadata":{"id":"BPX9eFrxnuAx","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:21.923965Z","iopub.execute_input":"2025-10-28T18:53:21.924285Z","iopub.status.idle":"2025-10-28T18:53:21.970817Z","shell.execute_reply.started":"2025-10-28T18:53:21.924261Z","shell.execute_reply":"2025-10-28T18:53:21.969590Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Load into Storage\n\nWe define a **docstore** to hold all nodes, and a **VectorStoreIndex** containing only the leaf-level nodes.\n","metadata":{"id":"FaacdYKdnzo7"}},{"cell_type":"code","source":"# define storage context\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.core import StorageContext\nfrom llama_index.llms.openai import OpenAI\n\ndocstore = SimpleDocumentStore()\n\n# insert nodes into docstore\ndocstore.add_documents(nodes)\n\n# define storage context (will include vector store by default too)\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n","metadata":{"id":"bx2iWIcZnwty","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:27.786982Z","iopub.execute_input":"2025-10-28T18:53:27.787296Z","iopub.status.idle":"2025-10-28T18:53:27.962703Z","shell.execute_reply.started":"2025-10-28T18:53:27.787272Z","shell.execute_reply":"2025-10-28T18:53:27.961509Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\nfrom google.colab import userdata\n","metadata":{"id":"qyPnlsDEwfuG","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:30.994714Z","iopub.execute_input":"2025-10-28T18:53:30.995668Z","iopub.status.idle":"2025-10-28T18:53:31.041999Z","shell.execute_reply.started":"2025-10-28T18:53:30.995638Z","shell.execute_reply":"2025-10-28T18:53:31.041011Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"HF_TOKEN_read\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:06:59.086338Z","iopub.execute_input":"2025-10-28T19:06:59.087157Z","iopub.status.idle":"2025-10-28T19:06:59.372587Z","shell.execute_reply.started":"2025-10-28T19:06:59.087122Z","shell.execute_reply":"2025-10-28T19:06:59.371515Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n\n# # Replace your existing LLM setup with this\n# llm = HuggingFaceInferenceAPI(\n#     model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n#     token=secret_value,  # Get a free token from https://huggingface.co/settings/tokens if needed\n#     temperature=0.7,  # Adjust as needed, based on your original code\n#     max_tokens=256,   # Adjust as needed\n#     context_window=3900  # Adjust if your original had this\n# )\n\nllm = HuggingFaceInferenceAPI( \n    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n    temperature=0.7, \n    max_tokens=100, \n    token=secret_value, \n    provider=\"together\", \n)\n","metadata":{"id":"SacJ82eawc0u","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:21.979736Z","iopub.execute_input":"2025-10-28T19:59:21.980139Z","iopub.status.idle":"2025-10-28T19:59:22.116011Z","shell.execute_reply.started":"2025-10-28T19:59:21.980115Z","shell.execute_reply":"2025-10-28T19:59:22.114970Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Set up the embedding model to use free local embeddings\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n# Settings.embed_model = HuggingFaceEmbedding(\n#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n# )\n\n# Replace your existing embed_model setup with this\nembed_model = HuggingFaceEmbedding(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n)\n\n# If using global Settings in LlamaIndex\nSettings.embed_model = embed_model\n","metadata":{"id":"VvAx9AlOsEUj","colab":{"base_uri":"https://localhost:8080/","height":500,"referenced_widgets":["cefbbbe553b645698eaa8dda7c3008d6","998f3fd94aaa4298a0069d2d5d420b07","dbb18e8fe1774513bb27cbc0136da538","3a3bee5d4dff48ad8022c0d8b3a30cb7","ab8e7ebdab064f218aa88e9ff30e0f8a","f2fbbb1561c345368d787986b6afe5be","79d52007d4ee4f099e46890bfb6660bc","5ec262d71cef453fa3d382872c2eb374","9af898ce959f4c8a970634fd00bd582c","01f7cd8f2a964d1896929fe0976de890","4f376c3e0fad41f382a548e13863f00d","3977d620ff6741cbb56483a825200d83","bcdb1464703a44a6b1be9ee7b7a74fc1","a512aab5ab8f4c3485094a40a281e333","51cf8aff671c45e28eecef0bec2d9dee","0af7d03c0e0845fe8229cbb22c0e2e83","a02e272164784f4ea6de3845526b223f","dddaa3a0946b42ccbc92171e4628dde5","f741c47b4cbb4b6aad8604e674cf1c8f","4271278572bf4da7bd6b0da27b2e1084","661fe76f6a1947c4bc29057377eb44a2","48347f58c1b640729c9f486743e37fdc","7b428c606d4140159e458f0255edff60","72dfcf808e4041fe985de9594b696fb3","2511ea2c8fb84b619ba8f68491578bdf","2ce64ae9edae4ee6979daf7863053c9a","44b7cd79d8de45c79f0879016acce35c","f8d0838c555b4960981fdde535b6d429","3c02c966c5c14fa5a3e155acbc7c6e83","d9d94100f8004bf8a086313d72b407ea","477e55c510104f82ac648057d1de35ef","ef8dfe24622c4161bc6b039deb26abf3","426189a8dba248e295bc3fd618240097","bd80e42e170d4f349d7f7c8112b03d04","12243ffd642646fbba24f545671b118f","3531086edebf4f2ebf2b117757945375","aed92eeb51e8438e81956d4d47748d96","969ec5ffce6c4f9294ab56295205e200","5f7d9e40dd4f4c908c1760047bacdaef","be3e6e8676024c65965702938e6756b5","ae69bcf0ccbc4ead818cce8e26df5596","c93d7839400f439b8d95287f2bc0dff6","e3f26730f7074ea7961baa64b4ff5529","f2e247407bab4aa494025184886c8f29","79cc4d78ff394ea58653ade177337947","ebf2eab83dba433eb6c89cd3346272f0","dacf121b87404743a1dc553c28afc2b0","b626cf6b716b4ebbb86170d354d13371","5cb9c24e575c4baeb3d246380ef5c5ea","c900e197c40a4384921fbaea52193cde","6abf1c02ba4d4cdca49f47e60aa5ae8a","ebd3b4b8286b43ce9552fef0095d4b02","60320a3988c04697bd30df89b43b195b","e99d35e66a5d4ca785d65c9b5f5d90bb","e9103097df354084b42d661bf2f594cb","26069878285b4f66907e3082509ebe75","68129bd9ae7e4d0abcd42306bd663cdb","6f91fa8264f44747b17a80f8b9700a86","ca704cc2c2d5443f87ec57d1d9c978de","4e33e14c02af448aa408ed977bcbcf4e","abb0bf1c00f24b4ca9f13b4aaeb85c78","743fbda77a8a4d19be47b9f76e1959f2","6c6ae20f4cc241ce8b6ea1c78150c398","32e02f3ac14943c0991569a6ed1ef21d","23c259b1d82c4194a2129d9c34504693","8fa8d25a4c4a45c498d8cf34619d871f","5bd64a67b60443609fd247c73088ed83","fdc4b763a2144d6ca131e2845f8b886a","23487d12ff4a4f008a15a4281c996bbd","44ecd1d2e5aa405c9b9107a4fe30829c","1f432ff251d44b83821e106dd1a016b7","38d8105019b643ef8af9581e586aac30","8e5e527788534b33abf3c184610b8088","465535896bd4418db215dc2058d1f75e","7ac08c0c9203400dbb52d879df29a6bd","3120eed398d949e4a333f2a497297c08","3e8fca6c1f624721b5d012298d8cef96","36e469a0744d464d859753e0c74be550","891db1d269904eaeaf93dfbb7c30999c","b9f403017d9b48eda2494f6c5b9d3cb8","0cfb6db8a58349cfade219485f59b2da","19811d100c62463dbbc15bbb2f8bff61","4712c83bfe1c4a4795bebe6398053637","83afa45cc9784606b8ff1df7a94de136","66af4157bff14b3493e157a04c6f40e0","9922a9bc0b654bab9d31f17bd7502189","ff9b76b75d19415f8534104e722783f9","a71d7a350f744b8facbe3e998bcc5eab","7a12fb73b82841e19265cb44130668c3","f1a01adc48d64cff85420d2e06942808","11a5411719144431b908289a8fb83440","3c667b6ebb4e49479a67ba74b554b65c","e5fad7f07df943e4b418b5077546f7cd","4d591e4ee694408296484dd598252d44","23e1ce7086a94fa8939a35ff0efcd7cd","fb31b6dfb87e481e84d04eb3032ab1cf","c03c4da096654d62acc92f3b9e926016","03b3402747114503a720616ef11b04c5","1a7974de8c284a0896004a4207d5edd1","efa2c6f305974c4ebb601da810efa00d","3096aaf5fc6f4bb7863a1f05870c73fb","6ec44078a3a046c78599724990784d5f","ef5b23202ed14f2d92b67cbcd144b633","84e5327462d94e69acb59c2d2d0a1126","d35346f6f44b4d3383ebc0dd4de0cd21","0cd71651498c498e9fc46f8e8175ba8d","e35265a7d2704665b9685c648344f578","c9dd0dbbd0184ba48501fabec0253665","131af91b7498467aa3f83529ac8556d8","0d86cc899f6746eda67d26875f99710c","dee80b61f04a492c9bbb683169c1b04f","f714666c4f8c4ef7bcc4a362f8c1129a","c6a68f3620a9430295141c00d83b0212","3f9b0059fecb4f7dbaafbce6d950a551","1b5a31801e9b41478e376785f0158a17","9954d2aa27f04276874ce82d6be17330","4a992513893f4b42977fd5e1529198d0","64b2630be685411f911033395214ee86","e231c4ecb129463a892a93593e2a8d57","90b7ee59e94e4a3c87ce3a7fc3853b85","d528c08d0e5245379f6fb094ca97f80a"]},"outputId":"3c21f929-639a-48e7-e2df-8b17d3902bc1","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:53:37.973446Z","iopub.execute_input":"2025-10-28T18:53:37.973766Z","iopub.status.idle":"2025-10-28T18:54:31.432433Z","shell.execute_reply.started":"2025-10-28T18:53:37.973744Z","shell.execute_reply":"2025-10-28T18:54:31.431420Z"}},"outputs":[{"name":"stderr","text":"2025-10-28 18:53:54.186196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761677634.417695      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761677634.486419      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b25d88f72864266a6c454de6d97912a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd01fbd877645b5aa40adae2aeb5a13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb9a14697ed448c9d2b0c5974fe68af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f9d19fa5f741a89d4a71d36ac4abf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a74861b5c042f4843166bfa6f2636b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2bcf19aaafb41969e72915dda3aa225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b45277bd15d4b9dbb6f6c4cacf5fbe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed124ce814e425687422455534f44cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d06d0311d4834eb1a541ab6bea822a5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65511bfddb547e3a201e566cd5f7578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa5824c6ef640828aa3b74115459647"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from llama_index.core import VectorStoreIndex, Settings\n\nbase_index = VectorStoreIndex(\n    leaf_nodes,\n    storage_context=storage_context,\n)\n","metadata":{"id":"jbhVnOPitRqP","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:54:40.774123Z","iopub.execute_input":"2025-10-28T18:54:40.774502Z","iopub.status.idle":"2025-10-28T18:57:09.709625Z","shell.execute_reply.started":"2025-10-28T18:54:40.774476Z","shell.execute_reply":"2025-10-28T18:57:09.708798Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# # You still need OpenAI API key for the LLM (GPT-3.5-turbo)\n# import os\n# from google.colab import userdata\n\n# os.environ[\"mytestkey\"] = userdata.get(\"mytestkey\")\n","metadata":{"id":"l3ygZPY8rf08","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:23.853526Z","iopub.execute_input":"2025-10-28T18:57:23.853825Z","iopub.status.idle":"2025-10-28T18:57:23.984413Z","shell.execute_reply.started":"2025-10-28T18:57:23.853804Z","shell.execute_reply":"2025-10-28T18:57:23.983309Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ## Load index into vector index\n# from llama_index.core import VectorStoreIndex\n# from google.colab import userdata\n# import os\n\n# os.environ[\"mytestkey\"] = userdata.get(\"mytestkey\")\n\n# base_index = VectorStoreIndex(\n#     leaf_nodes,\n#     storage_context=storage_context,\n# )\n","metadata":{"id":"JX-ad8Ngn_2n","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:24.230991Z","iopub.execute_input":"2025-10-28T18:57:24.231343Z","iopub.status.idle":"2025-10-28T18:57:24.357131Z","shell.execute_reply.started":"2025-10-28T18:57:24.231319Z","shell.execute_reply":"2025-10-28T18:57:24.356179Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Define Retriever\n","metadata":{"id":"g62WUjRRoDpz"}},{"cell_type":"code","source":"from llama_index.core.retrievers import AutoMergingRetriever\n","metadata":{"id":"mGd_SHLsoBSx","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:25.442817Z","iopub.execute_input":"2025-10-28T18:57:25.443501Z","iopub.status.idle":"2025-10-28T18:57:25.585252Z","shell.execute_reply.started":"2025-10-28T18:57:25.443476Z","shell.execute_reply":"2025-10-28T18:57:25.584310Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"base_retriever = base_index.as_retriever(similarity_top_k=6)\nretriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n","metadata":{"id":"AQVshZvttIvX","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:28.571526Z","iopub.execute_input":"2025-10-28T18:57:28.572217Z","iopub.status.idle":"2025-10-28T18:57:28.696577Z","shell.execute_reply.started":"2025-10-28T18:57:28.572186Z","shell.execute_reply":"2025-10-28T18:57:28.695694Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# query_str = \"What were some lessons learned from red-teaming?\"\n# query_str = \"Can you tell me about the key concepts for safety finetuning\"\nquery_str = (\n    \"What could be the potential outcomes of adjusting the amount of safety\"\n    \" data used in the RLHF stage?\"\n)\n\nnodes = retriever.retrieve(query_str)\nbase_nodes = base_retriever.retrieve(query_str)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mkex0O27upCi","outputId":"ee8276e1-d429-43a5-8423-cf1a8eba993d","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:30.710528Z","iopub.execute_input":"2025-10-28T18:57:30.710869Z","iopub.status.idle":"2025-10-28T18:57:31.119723Z","shell.execute_reply.started":"2025-10-28T18:57:30.710842Z","shell.execute_reply":"2025-10-28T18:57:31.118360Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"len(nodes)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNB1lmClusu_","outputId":"cb081b25-6206-44f3-ab17-5a04c4a17238","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:36.849055Z","iopub.execute_input":"2025-10-28T18:57:36.849892Z","iopub.status.idle":"2025-10-28T18:57:36.976769Z","shell.execute_reply.started":"2025-10-28T18:57:36.849864Z","shell.execute_reply":"2025-10-28T18:57:36.976005Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"len(base_nodes)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smuIOmy_uxwB","outputId":"6143dc45-45a2-4bdc-9757-d764686775e2","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:40.826568Z","iopub.execute_input":"2025-10-28T18:57:40.826882Z","iopub.status.idle":"2025-10-28T18:57:40.952855Z","shell.execute_reply.started":"2025-10-28T18:57:40.826859Z","shell.execute_reply":"2025-10-28T18:57:40.951962Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"from llama_index.core.response.notebook_utils import display_source_node\n\nfor node in nodes:\n    display_source_node(node, source_length=10000)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"CKeijqZDuzrW","outputId":"089ec65e-b2ef-47b9-caab-d28f66c55385","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:43.479691Z","iopub.execute_input":"2025-10-28T18:57:43.480024Z","iopub.status.idle":"2025-10-28T18:57:43.624635Z","shell.execute_reply.started":"2025-10-28T18:57:43.479999Z","shell.execute_reply":"2025-10-28T18:57:43.623766Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** a45aa4a1-b670-46dd-a436-206ff0f8a45f<br>**Similarity:** 0.6596026300549541<br>**Text:** A clear cluster appears on the top-left\ncorner suggesting the improvements of model safety. On the right side, we do not observe any gathering\npattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score\ndistribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training\ndata, the addition of an additional stage of safety mitigation does not negatively impact model performance\non helpfulness to any notable degradation. A qualitative example is shown in Table 12.\nImpact of Safety Data Scaling.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 95943b7c-988e-47d5-9b1e-407c1cf3e762<br>**Similarity:** 0.6426033304077619<br>**Text:** A qualitative example is shown in Table 12.\nImpact of Safety Data Scaling.\nA tension between helpfulness and safety of LLMs has been observed in\nprevious studies (Bai et al., 2022a). To better understand how the addition of safety training data affects\ngeneral model performance, especially helpfulness, we investigate the trends in safety data scaling by\nadjusting the amount of safety data used in the RLHF stage.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 2e0186d7-0a9b-4ac4-9124-a1e5426a9390<br>**Similarity:** 0.6253564955351361<br>**Text:** If you have any questions or concerns, please feel\nfree to ask and I will do my best to assist you in a\nresponsible and safe manner. Let’s all strive to use\nthe internet and email for positive and beneficial pur-\nposes. Is there something else I can help you with?\nTable 12: Example of safer response after safety RLHF. Left: response is generated by an early version of\nthe model (SFT-v2). Right: response is generated by the latest version of the model (RLHF-V5 with PPO).\n25<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 42523f92-81b6-47ce-8e9f-b00bb190d38e<br>**Similarity:** 0.557578081415944<br>**Text:** 10%\nSafety Data Pct. 25%\nSafety Data Pct. 50%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSafety Reward Model Score\nSafety Data Pct. 100%\nFigure 15: Safety data scaling trends. Left: as we increase the amount of safety data in model training, the\nmean safety RM score improves significantly while the helpfulness counterpart remains relatively stable.\nRight: the left tail of safety RM scores (i.e., most unsafe responses) gradually disappears with the addition of\nmore safety training data.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 368c87e8-a3c3-4df8-bb39-29cdff8d461c<br>**Similarity:** 0.5206550180373152<br>**Text:** Selected\nRejected\nOriginal Safety RM Score\nScore increase with CD\n(b) Targeted Context Distillation.\nFigure 16: Context distillation analysis. Left: Distribution of safety RM scores from the base model, when\nadding a generic preprompt, and when adding a preprompt based on the risk category with tailored answer\ntemplate. While a generic preprompt increases safety RM scores, a preprompt with tailored answer template\nhelps even more.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 69a55d15-ea5c-4094-9488-1abae4173dc0<br>**Similarity:** 0.5043828202935488<br>**Text:** We also list two\nqualitative examples where safety and helpfulness reward models don’t agree with each other in Table 35.\nA.4.2\nQualitative Results on Safety Data Scaling\nIn Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner.\nHere we showcase a few samples to qualitatively examine the evolution of model behavior when we scale\nsafety data in Tables 36, 37, and 38.<br>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"for node in base_nodes:\n    display_source_node(node, source_length=10000)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"_fkcMWW1u1cu","outputId":"3149e9ac-caba-49b2-9984-78f67cc469f5","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:48.647766Z","iopub.execute_input":"2025-10-28T18:57:48.648076Z","iopub.status.idle":"2025-10-28T18:57:48.782612Z","shell.execute_reply.started":"2025-10-28T18:57:48.648055Z","shell.execute_reply":"2025-10-28T18:57:48.781617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** a45aa4a1-b670-46dd-a436-206ff0f8a45f<br>**Similarity:** 0.6596026300549541<br>**Text:** A clear cluster appears on the top-left\ncorner suggesting the improvements of model safety. On the right side, we do not observe any gathering\npattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score\ndistribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training\ndata, the addition of an additional stage of safety mitigation does not negatively impact model performance\non helpfulness to any notable degradation. A qualitative example is shown in Table 12.\nImpact of Safety Data Scaling.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 95943b7c-988e-47d5-9b1e-407c1cf3e762<br>**Similarity:** 0.6426033304077619<br>**Text:** A qualitative example is shown in Table 12.\nImpact of Safety Data Scaling.\nA tension between helpfulness and safety of LLMs has been observed in\nprevious studies (Bai et al., 2022a). To better understand how the addition of safety training data affects\ngeneral model performance, especially helpfulness, we investigate the trends in safety data scaling by\nadjusting the amount of safety data used in the RLHF stage.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 2e0186d7-0a9b-4ac4-9124-a1e5426a9390<br>**Similarity:** 0.6253564955351361<br>**Text:** If you have any questions or concerns, please feel\nfree to ask and I will do my best to assist you in a\nresponsible and safe manner. Let’s all strive to use\nthe internet and email for positive and beneficial pur-\nposes. Is there something else I can help you with?\nTable 12: Example of safer response after safety RLHF. Left: response is generated by an early version of\nthe model (SFT-v2). Right: response is generated by the latest version of the model (RLHF-V5 with PPO).\n25<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 42523f92-81b6-47ce-8e9f-b00bb190d38e<br>**Similarity:** 0.557578081415944<br>**Text:** 10%\nSafety Data Pct. 25%\nSafety Data Pct. 50%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSafety Reward Model Score\nSafety Data Pct. 100%\nFigure 15: Safety data scaling trends. Left: as we increase the amount of safety data in model training, the\nmean safety RM score improves significantly while the helpfulness counterpart remains relatively stable.\nRight: the left tail of safety RM scores (i.e., most unsafe responses) gradually disappears with the addition of\nmore safety training data.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 368c87e8-a3c3-4df8-bb39-29cdff8d461c<br>**Similarity:** 0.5206550180373152<br>**Text:** Selected\nRejected\nOriginal Safety RM Score\nScore increase with CD\n(b) Targeted Context Distillation.\nFigure 16: Context distillation analysis. Left: Distribution of safety RM scores from the base model, when\nadding a generic preprompt, and when adding a preprompt based on the risk category with tailored answer\ntemplate. While a generic preprompt increases safety RM scores, a preprompt with tailored answer template\nhelps even more.<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Node ID:** 69a55d15-ea5c-4094-9488-1abae4173dc0<br>**Similarity:** 0.5043828202935488<br>**Text:** We also list two\nqualitative examples where safety and helpfulness reward models don’t agree with each other in Table 35.\nA.4.2\nQualitative Results on Safety Data Scaling\nIn Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner.\nHere we showcase a few samples to qualitatively examine the evolution of model behavior when we scale\nsafety data in Tables 36, 37, and 38.<br>"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Plug it into Query Engine","metadata":{"id":"jrJEl6VGu8gT"}},{"cell_type":"code","source":"from llama_index.core.query_engine import RetrieverQueryEngine\n","metadata":{"id":"YHT1oojtu4vO","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:57:53.616354Z","iopub.execute_input":"2025-10-28T18:57:53.616709Z","iopub.status.idle":"2025-10-28T18:57:53.743622Z","shell.execute_reply.started":"2025-10-28T18:57:53.616688Z","shell.execute_reply":"2025-10-28T18:57:53.742468Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Pass the free LLM to the query engine\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever,\n    llm=llm  # Add this parameter\n)\n\nbase_query_engine = RetrieverQueryEngine.from_args(\n    base_retriever,\n    llm=llm  # Add this parameter\n)\n","metadata":{"id":"ZFAGgpHNu_Cy","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:32.271588Z","iopub.execute_input":"2025-10-28T19:59:32.272270Z","iopub.status.idle":"2025-10-28T19:59:32.401356Z","shell.execute_reply.started":"2025-10-28T19:59:32.272242Z","shell.execute_reply":"2025-10-28T19:59:32.400389Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"response = query_engine.query(query_str)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":522},"id":"o4PMQiHgvAlo","outputId":"a2b2e885-ae67-4664-d8ad-35a19e86d559","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:33.775829Z","iopub.execute_input":"2025-10-28T19:59:33.776414Z","iopub.status.idle":"2025-10-28T19:59:35.374671Z","shell.execute_reply.started":"2025-10-28T19:59:33.776371Z","shell.execute_reply":"2025-10-28T19:59:35.373927Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"print(str(response))\n","metadata":{"id":"himp4bwwvCOl","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:38.844738Z","iopub.execute_input":"2025-10-28T19:59:38.845071Z","iopub.status.idle":"2025-10-28T19:59:38.973850Z","shell.execute_reply.started":"2025-10-28T19:59:38.845045Z","shell.execute_reply":"2025-10-28T19:59:38.972835Z"}},"outputs":[{"name":"stdout","text":"Adjusting the amount of safety data used in the RLHF stage could lead to a significant improvement in the mean safety score of the model while keeping the helpfulness score relatively stable. Additionally, the addition of more safety training data would gradually reduce the occurrence of very unsafe responses, as indicated by the disappearance of the left tail in the distribution of safety scores.\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"base_response = base_query_engine.query(query_str)\n","metadata":{"id":"mKqodIQ6vI2-","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:46.147331Z","iopub.execute_input":"2025-10-28T19:59:46.148477Z","iopub.status.idle":"2025-10-28T19:59:47.566836Z","shell.execute_reply.started":"2025-10-28T19:59:46.148434Z","shell.execute_reply":"2025-10-28T19:59:47.566070Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"print(str(base_response))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T19:59:50.140126Z","iopub.execute_input":"2025-10-28T19:59:50.140996Z","iopub.status.idle":"2025-10-28T19:59:50.268510Z","shell.execute_reply.started":"2025-10-28T19:59:50.140964Z","shell.execute_reply":"2025-10-28T19:59:50.267391Z"}},"outputs":[{"name":"stdout","text":"Adjusting the amount of safety data used in the RLHF stage could lead to improvements in the safety of the model without significantly impacting its helpfulness. Increasing the safety data can enhance the mean safety score while keeping the helpfulness score relatively stable. Additionally, the distribution of the most unsafe responses could be reduced as more safety training data is introduced.\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"# Evaluation\n\n## Quantitative Evaluation: Hierarchical vs. Baseline Retriever\n\nWe perform a **quantitative comparison** to assess the performance of the **hierarchical retriever** against the **baseline retriever**.\n\n> **⚠️ WARNING**: This evaluation can be **expensive**, particularly when using **GPT-4**.  \n> Proceed with caution and **adjust the sample size** to align with your budget.","metadata":{}},{"cell_type":"code","source":"from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\nfrom llama_index.llms.openai import OpenAI\nimport nest_asyncio\n\nnest_asyncio.apply()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T20:02:18.606489Z","iopub.execute_input":"2025-10-28T20:02:18.606812Z","iopub.status.idle":"2025-10-28T20:02:18.734025Z","shell.execute_reply.started":"2025-10-28T20:02:18.606790Z","shell.execute_reply":"2025-10-28T20:02:18.733237Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# NOTE: run this if the dataset isn't already saved\n# Note: we only generate from the first 20 nodes, since the rest are references\neval_llm = HuggingFaceInferenceAPI( \n    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n    temperature=0.7, \n    max_tokens=100, \n    token=secret_value, \n    provider=\"together\", \n)\ndataset_generator = DatasetGenerator(\n    root_nodes[:20],\n    llm=eval_llm,\n    show_progress=True,\n    num_questions_per_chunk=3,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T20:03:02.232085Z","iopub.execute_input":"2025-10-28T20:03:02.232454Z","iopub.status.idle":"2025-10-28T20:03:02.363156Z","shell.execute_reply.started":"2025-10-28T20:03:02.232428Z","shell.execute_reply":"2025-10-28T20:03:02.362024Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2369020831.py:10: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n  dataset_generator = DatasetGenerator(\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T20:03:03.528351Z","iopub.execute_input":"2025-10-28T20:03:03.529046Z","iopub.status.idle":"2025-10-28T20:03:04.119201Z","shell.execute_reply.started":"2025-10-28T20:03:03.529016Z","shell.execute_reply":"2025-10-28T20:03:04.117589Z"}},"outputs":[{"name":"stderr","text":"\n\n\n\n\n\n\n\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2465858903.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mdataset_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magenerate_dataset_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/dataset_generation.py\u001b[0m in \u001b[0;36magenerate_dataset_from_nodes\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    308\u001b[0m     ) -> QueryResponseDataset:\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m\"\"\"Generates questions for each document.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         return await self._agenerate_dataset(\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/dataset_generation.py\u001b[0m in \u001b[0;36m_agenerate_dataset\u001b[0;34m(self, nodes, num, generate_response)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0msummary_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masync_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mquery_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                                  total=total, **tqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                                  total=total, **tqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m_wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# Dummy value from _on_timeout().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# May raise f.exception().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtodo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36mwrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[1;32m     75\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/base_query_engine.py\u001b[0m in \u001b[0;36maquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         dispatcher.event(\n\u001b[1;32m     58\u001b[0m             \u001b[0mQueryEndEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\u001b[0m in \u001b[0;36m_aquery\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             response = await self._response_synthesizer.asynthesize(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/base.py\u001b[0m in \u001b[0;36masynthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY_STR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         ) as event:\n\u001b[0;32m--> 300\u001b[0;31m             response_str = await self.aget_response(\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mquery_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 text_chunks=[\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\u001b[0m in \u001b[0;36maget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     ) -> RESPONSE_TEXT_TYPE:\n\u001b[1;32m     22\u001b[0m         \u001b[0mcompact_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_compact_text_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         return await super().aget_response(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mquery_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtext_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompact_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/refine.py\u001b[0m in \u001b[0;36maget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;31m# if this is the first chunk, and text chunk already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m# is an answer, then return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 response = await self._agive_response_single(\n\u001b[0m\u001b[1;32m    367\u001b[0m                     \u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mresponse_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/refine.py\u001b[0m in \u001b[0;36m_agive_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_streaming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                     structured_response = await program.acall(\n\u001b[0m\u001b[1;32m    488\u001b[0m                         \u001b[0mcontext_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_text_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0mresponse_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/refine.py\u001b[0m in \u001b[0;36macall\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             answer = await self._llm.apredict(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/llm.py\u001b[0m in \u001b[0;36mapredict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_chat_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprompt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mchat_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0machat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/huggingface_api/base.py\u001b[0m in \u001b[0;36machat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             output: ChatCompletionOutput = await self._async_client.chat_completion(\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_huggingface_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         )\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_error_payload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_error_payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_async_yield_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             raise ClientResponseError(\n\u001b[0m\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mClientResponseError\u001b[0m: 402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions'"],"ename":"ClientResponseError","evalue":"402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions'","output_type":"error"}],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}